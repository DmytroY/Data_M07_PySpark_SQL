{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b689377-ce29-450a-93e7-2f218e517da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/21 13:41:07 WARN SSLSocketFactoryEx: Failed to load OpenSSL. Falling back to the JSSE default.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%run 00_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cac9879d-64d7-443a-8646-6d5557b37fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sequence, explode, to_date, datediff, monotonically_increasing_id, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4551db43-d27e-412e-b346-d8ed9f0e7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more than 7 days stay\n",
    "over7d_notGrupped = ex_df.filter(datediff(to_date(\"srch_co\"), to_date(\"srch_ci\")) > 7)\n",
    "over7d = over7d_notGrupped.groupBy('hotel_id', 'srch_ci', 'srch_co').count()\n",
    "# add stay_id here\n",
    "over7d = over7d.withColumn('stay_id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1fbf39cf-73dc-4d64-bfab-a5c06f506605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "679196"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expand date range to records with exact dates and year_month field\n",
    "over7 = over7d.select(\n",
    "    'stay_id',\n",
    "    'hotel_id',\n",
    "    'srch_ci',\n",
    "    'srch_co',\n",
    "    explode(sequence(to_date('srch_ci'),to_date('srch_co'))).alias('date')\n",
    ")\n",
    "over7.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c0b67c3-a9d7-4365-8970-3da544a3d157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33280"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joid weather data, on hotel_id cover only 5% of records in \"stay over 7days\" data. Must find workaround\n",
    "\n",
    "over7weather = over7.join(hw_df, [over7.hotel_id == hw_df.id, over7.date == hw_df.wthr_date] , \"inner\")\n",
    "over7weather.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd2fe10c-8dac-444c-99b0-f63e3138d882",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column avg_tmpr_c#1 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# let's update over7 with geohash and join weather data on geohash and date.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m over7geo \u001b[38;5;241m=\u001b[39m over7\u001b[38;5;241m.\u001b[39mjoin(hw_df, over7\u001b[38;5;241m.\u001b[39mhotel_id \u001b[38;5;241m==\u001b[39m hw_df\u001b[38;5;241m.\u001b[39mid, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(over7[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m], hw_df\u001b[38;5;241m.\u001b[39mgeoHash)\n\u001b[0;32m----> 3\u001b[0m over7weather \u001b[38;5;241m=\u001b[39m \u001b[43mover7geo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhw_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mover7geo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeoHash\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhw_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeoHash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mover7geo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhw_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwthr_date\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mover7geo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhw_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_tmpr_c\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m over7weather\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:3223\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m \n\u001b[1;32m   3181\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column avg_tmpr_c#1 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check."
     ]
    }
   ],
   "source": [
    "# let's update over7 with geohash and join weather data on geohash and date.\n",
    "over7geo = over7.join(hw_df, over7.hotel_id == hw_df.id, \"inner\").select(over7['*'], hw_df.geoHash)\n",
    "over7weather = over7geo.join(hw_df, [over7geo.geoHash == hw_df.geoHash, over7geo.date == hw_df.wthr_date] , \"inner\").select(over7geo['*'], hw_df.avg_tmpr_c)\n",
    "over7weather.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273a145-e278-4b6e-a6e5-61978492ea9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4803b152-9dc6-48bc-8c29-325a37e7ffce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stay_id: long (nullable = false)\n",
      " |-- hotel_id: long (nullable = true)\n",
      " |-- srch_ci: string (nullable = true)\n",
      " |-- srch_co: string (nullable = true)\n",
      " |-- date: date (nullable = false)\n",
      " |-- address: string (nullable = true)\n",
      " |-- avg_tmpr_c: double (nullable = true)\n",
      " |-- avg_tmpr_f: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- geoHash: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- wthr_date: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "over7weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "42deb83a-08c0-46b4-8954-4fd817c9580e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+----------+----------+-------------+\n",
      "|stay_id|    hotel_id|   srch_ci|   srch_co|      date|avg_tmpr_stay|\n",
      "+-------+------------+----------+----------+----------+-------------+\n",
      "|      1|824633720836|2017-09-12|2017-10-28|2017-09-20|         23.4|\n",
      "|      1|824633720836|2017-09-12|2017-10-28|2017-09-27|         23.5|\n",
      "|   1720|824633720836|2017-08-17|2017-08-28|2017-08-27|         17.6|\n",
      "|   1987|824633720836|2017-09-21|2017-10-02|2017-09-27|         23.6|\n",
      "|  15676|824633720836|2017-08-24|2017-09-02|2017-08-27|         17.6|\n",
      "|  28339|824633720836|2017-08-17|2017-08-31|2017-08-27|         17.6|\n",
      "|  30454|824633720836|2017-08-23|2017-08-31|2017-08-27|         17.6|\n",
      "|  31707|824633720836|2017-09-18|2017-09-26|2017-09-20|         23.4|\n",
      "|  32187|824633720836|2017-09-16|2017-09-26|2017-09-20|         23.4|\n",
      "|  48791|824633720836|2017-08-22|2017-08-30|2017-08-27|         17.6|\n",
      "|  49459|824633720836|2016-10-22|2016-10-30|2016-10-25|          7.7|\n",
      "|  53072|824633720836|2016-10-24|2016-11-01|2016-10-25|          7.7|\n",
      "+-------+------------+----------+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# temperature difference last and first day of stay) and, average temperature during stay\n",
    "window = Window.partitionBy('stay_id').orderBy('date')\n",
    "result = over7weather.withColumn(\"avg_tmpr_stay\", avg('avg_tmpr_c').over(window))\n",
    "result.select(\n",
    "    'stay_id',\n",
    "    'hotel_id',\n",
    "    'srch_ci',\n",
    "    'srch_co',\n",
    "    'date',\n",
    "    'avg_tmpr_stay'\n",
    ").where('hotel_id = 824633720836').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f47b7920-bb1a-471c-ae44-8b112fc48755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date_time: string (nullable = true)\n",
      " |-- site_name: integer (nullable = true)\n",
      " |-- posa_continent: integer (nullable = true)\n",
      " |-- user_location_country: integer (nullable = true)\n",
      " |-- user_location_region: integer (nullable = true)\n",
      " |-- user_location_city: integer (nullable = true)\n",
      " |-- orig_destination_distance: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- is_mobile: integer (nullable = true)\n",
      " |-- is_package: integer (nullable = true)\n",
      " |-- channel: integer (nullable = true)\n",
      " |-- srch_ci: string (nullable = true)\n",
      " |-- srch_co: string (nullable = true)\n",
      " |-- srch_adults_cnt: integer (nullable = true)\n",
      " |-- srch_children_cnt: integer (nullable = true)\n",
      " |-- srch_rm_cnt: integer (nullable = true)\n",
      " |-- srch_destination_id: integer (nullable = true)\n",
      " |-- srch_destination_type_id: integer (nullable = true)\n",
      " |-- hotel_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d19dc38-968e-4a92-a811-1197b7ab6c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
