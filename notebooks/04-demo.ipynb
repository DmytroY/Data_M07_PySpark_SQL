{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "## This demonstrates how perform choosing first/last record data in window function partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stay_id: long (nullable = true)\n",
      " |-- ci: string (nullable = true)\n",
      " |-- co: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- tmpr: long (nullable = true)\n",
      "\n",
      "+-------+----------+----------+----------+----+\n",
      "|stay_id|ci        |co        |date      |tmpr|\n",
      "+-------+----------+----------+----------+----+\n",
      "|1      |2017-09-12|2017-10-28|2017-09-12|0   |\n",
      "|1      |2017-09-12|2017-10-28|2017-09-13|41  |\n",
      "|1      |2017-09-12|2017-10-28|2017-09-14|42  |\n",
      "|1      |2017-09-12|2017-10-28|2017-09-15|43  |\n",
      "|1      |2017-09-12|2017-10-28|2017-09-16|44  |\n",
      "|1      |2017-09-12|2017-10-28|2017-09-17|5   |\n",
      "|2      |2017-09-12|2017-10-28|2017-09-15|101 |\n",
      "|2      |2017-09-12|2017-10-28|2017-09-16|102 |\n",
      "|2      |2017-09-12|2017-10-28|2017-09-17|103 |\n",
      "+-------+----------+----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myData =   ((1, \"2017-09-12\", \"2017-10-28\", \"2017-09-12\", 0), \\\n",
    "            (1, \"2017-09-12\", \"2017-10-28\", \"2017-09-13\", 41), \\\n",
    "            (1, \"2017-09-12\", \"2017-10-28\", \"2017-09-14\", 42), \\\n",
    "            (1, \"2017-09-12\", \"2017-10-28\", \"2017-09-15\", 43), \\\n",
    "            (1, \"2017-09-12\", \"2017-10-28\", \"2017-09-16\", 44), \\\n",
    "            (1, \"2017-09-12\", \"2017-10-28\", \"2017-09-17\", 5), \\\n",
    "            (2, \"2017-09-12\", \"2017-10-28\", \"2017-09-15\", 101), \\\n",
    "            (2, \"2017-09-12\", \"2017-10-28\", \"2017-09-16\", 102), \\\n",
    "            (2, \"2017-09-12\", \"2017-10-28\", \"2017-09-17\", 103))\n",
    " \n",
    "columns= ['stay_id','ci','co','date','tmpr']\n",
    "df = spark.createDataFrame(data = myData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col, lag, avg, first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+---------+------------------+\n",
      "|stay_id|        ci|        co|first_tmpr|last_tmpr|          avg_tmpr|\n",
      "+-------+----------+----------+----------+---------+------------------+\n",
      "|      1|2017-09-12|2017-10-28|         0|        5|29.166666666666668|\n",
      "|      2|2017-09-12|2017-10-28|       101|      103|             102.0|\n",
      "+-------+----------+----------+----------+---------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "windowSpec  = Window.partitionBy(\"stay_id\").orderBy(col(\"date\"))\n",
    "windowSpecDesc  = Window.partitionBy(\"stay_id\").orderBy(col(\"date\").desc())\n",
    "windowSpecAgg  = Window.partitionBy(\"stay_id\")\n",
    "\n",
    "df.withColumn(\"first_tmpr\", first(\"tmpr\").over(windowSpec)) \\\n",
    "    .withColumn(\"last_tmpr\", first(\"tmpr\").over(windowSpecDesc)) \\\n",
    "    .withColumn(\"avg_tmpr\", avg(\"tmpr\").over(windowSpecAgg)) \\\n",
    "    .withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "    .where(\"row = 1\") \\\n",
    "    .select(\"stay_id\", \"ci\", \"co\", \"first_tmpr\", \"last_tmpr\", \"avg_tmpr\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "## This is Azure blob write/read test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"abfss://data@styakovdwesteurope.dfs.core.windows.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 23:49:18 WARN SSLSocketFactoryEx: Failed to load OpenSSL. Falling back to the JSSE default.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet(data_path  + \"/ouput/test1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+----+\n",
      "|stay_id|        ci|        co|      date|tmpr|\n",
      "+-------+----------+----------+----------+----+\n",
      "|      2|2017-09-12|2017-10-28|2017-09-15| 101|\n",
      "|      2|2017-09-12|2017-10-28|2017-09-16| 102|\n",
      "|      2|2017-09-12|2017-10-28|2017-09-17| 103|\n",
      "|      1|2017-09-12|2017-10-28|2017-09-12|   0|\n",
      "|      1|2017-09-12|2017-10-28|2017-09-13|  41|\n",
      "|      1|2017-09-12|2017-10-28|2017-09-14|  42|\n",
      "|      1|2017-09-12|2017-10-28|2017-09-15|  43|\n",
      "|      1|2017-09-12|2017-10-28|2017-09-16|  44|\n",
      "|      1|2017-09-12|2017-10-28|2017-09-17|   5|\n",
      "+-------+----------+----------+----------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_df = spark.read.parquet(data_path  + \"/ouput/test1.parquet\")\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "## This demonstrates SQL sintaxis magic in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">1</td></tr><tr><td>1</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
